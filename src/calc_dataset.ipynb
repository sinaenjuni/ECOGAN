{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize, Compose\n",
    "from utils.misc import print_h5py, elapsed_time\n",
    "import h5py\n",
    "from utils.confusion_matrix import ConfusionMatrix\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {'ori':'/shared_hdd/sin/data.h5py',\n",
    "         'gen':'/shared_hdd/sin/gen.h5py'}\n",
    "\n",
    "ids=[\"1r08m7xe\",\n",
    "\"3rczol1e\",\n",
    "\"3u337fao\",\n",
    "\"6gem8lca\",\n",
    "\"3p2vbr49\",\n",
    "\"i2mijyme\",\n",
    "\"iitr6wnc\",\n",
    "\"vpgfgriu\"]\n",
    "\n",
    "data_names=[\"CIFAR10_LT\",\n",
    "\"CIFAR10_LT\",\n",
    "\"CIFAR10_LT\",\n",
    "\"CIFAR10_LT\",\n",
    "\"FashionMNIST_LT\",\n",
    "\"FashionMNIST_LT\",\n",
    "\"FashionMNIST_LT\",\n",
    "\"FashionMNIST_LT\"]\n",
    "\n",
    "gen_query = lambda id, data_name : f'gen/{id}/{data_name}'\n",
    "ori_query = lambda data_name : f'ori/{data_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\n",
      "\t1r08m7xe\n",
      "\t\tCIFAR10_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(40500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(40500,)\n",
      "\t3p2vbr49\n",
      "\t\tFashionMNIST_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(49500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(49500,)\n",
      "\t3rczol1e\n",
      "\t\tCIFAR10_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(40500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(40500,)\n",
      "\t3u337fao\n",
      "\t\tCIFAR10_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(40500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(40500,)\n",
      "\t6gem8lca\n",
      "\t\tCIFAR10_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(40500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(40500,)\n",
      "\ti2mijyme\n",
      "\t\tFashionMNIST_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(49500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(49500,)\n",
      "\tiitr6wnc\n",
      "\t\tFashionMNIST_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(49500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(49500,)\n",
      "\tvpgfgriu\n",
      "\t\tFashionMNIST_LT\n",
      "\t\t\tdata\n",
      "\t\t\t(49500, 64, 64, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(49500,)\n",
      "ori\n",
      "\tCIFAR10_LT\n",
      "\t\ttest\n",
      "\t\t\tdata\n",
      "\t\t\t(10000, 32, 32, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(10000,)\n",
      "\t\ttrain\n",
      "\t\t\tdata\n",
      "\t\t\t(9500, 32, 32, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(9500,)\n",
      "\tFashionMNIST_LT\n",
      "\t\ttest\n",
      "\t\t\tdata\n",
      "\t\t\t(10000, 28, 28)\n",
      "\t\t\ttargets\n",
      "\t\t\t(10000,)\n",
      "\t\ttrain\n",
      "\t\t\tdata\n",
      "\t\t\t(10500, 28, 28)\n",
      "\t\t\ttargets\n",
      "\t\t\t(10500,)\n",
      "\tPlaces_LT\n",
      "\t\ttest\n",
      "\t\t\tdata\n",
      "\t\t\t(36500, 256, 256, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(36500,)\n",
      "\t\ttrain\n",
      "\t\t\tdata\n",
      "\t\t\t(62500, 256, 256, 3)\n",
      "\t\t\ttargets\n",
      "\t\t\t(62500,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('/shared_hdd/sin/gen.h5py', 'r') as file:\n",
    "    print_h5py(file)\n",
    "\n",
    "with h5py.File('/shared_hdd/sin/data.h5py', 'r') as file:\n",
    "    print_h5py(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5py_dataset(Dataset):\n",
    "    def __init__(self, path, query, transforms=None):\n",
    "        file = h5py.File(path, 'r')        \n",
    "        self.data = file[f'{query}/data'].astype(np.uint8)\n",
    "        self.targets = file[f'{query}/targets'].astype(np.uint8)\n",
    "        self.transforms = transforms\n",
    "        self.classes, self.num_classes = np.unique(self.targets, return_counts=True)\n",
    "        self.num_class = len(self.classes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.data[idx], self.targets[idx]\n",
    "        \n",
    "        if img.ndim == 2:\n",
    "            img = img[..., None].repeat(3, 2)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, label\n",
    "\n",
    "transforms = Compose([ToTensor(),\n",
    "                    Resize(64),\n",
    "                    Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "gen_dataset = H5py_dataset(path=paths['gen'], query=f'gen/{ids[args.data_seq]}/{data_names[args.data_seq]}', transforms=transforms)\n",
    "ori_dataset = H5py_dataset(path=paths['ori'], query=f'ori/{data_names[args.data_seq]}/train', transforms=transforms)\n",
    "\n",
    "train_dataset = ConcatDataset([ori_dataset, gen_dataset])\n",
    "test_dataset = H5py_dataset(path=paths['ori'], query=f'ori/{data_names[args.data_seq]}/test', transforms=transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100/1000(10.00%), time: 0:00:18, loss train: 1.8067075848579406, loss test: 2.2844077575055857, acc train: 0.571796875, acc test: 0.1667, acc best: 0.1667\n",
      "step: 200/1000(20.00%), time: 0:00:37, loss train: 1.1909965401887894, loss test: 2.2423489652102506, acc train: 0.85421875, acc test: 0.1903, acc best: 0.1903\n",
      "step: 300/1000(30.00%), time: 0:00:56, loss train: 0.8435622948408127, loss test: 2.2512957189656513, acc train: 0.8728125, acc test: 0.1971, acc best: 0.1971\n",
      "step: 400/1000(40.00%), time: 0:01:15, loss train: 0.6620379745960235, loss test: 2.2939400348482253, acc train: 0.8805677540777918, acc test: 0.1983, acc best: 0.1983\n",
      "step: 500/1000(50.00%), time: 0:01:34, loss train: 0.5553600159287453, loss test: 2.285524070262909, acc train: 0.883125, acc test: 0.2046, acc best: 0.2046\n",
      "step: 600/1000(60.00%), time: 0:01:52, loss train: 0.4775961628556252, loss test: 2.3138351817674274, acc train: 0.8946875, acc test: 0.1963, acc best: 0.2046\n",
      "step: 700/1000(70.00%), time: 0:02:12, loss train: 0.4390957948565483, loss test: 2.30719596373884, acc train: 0.897578125, acc test: 0.2052, acc best: 0.2052\n",
      "step: 800/1000(80.00%), time: 0:02:30, loss train: 0.41525676771998404, loss test: 2.303993683072585, acc train: 0.8987609786700126, acc test: 0.2077, acc best: 0.2077\n",
      "step: 900/1000(90.00%), time: 0:02:49, loss train: 0.3921199548244476, loss test: 2.317738597151599, acc train: 0.900703125, acc test: 0.206, acc best: 0.2077\n",
      "step: 1000/1000(100.00%), time: 0:03:07, loss train: 0.38105530083179473, loss test: 2.317732478244395, acc train: 0.899921875, acc test: 0.2084, acc best: 0.2084\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.gpus)\n",
    "# device = args.gpus\n",
    "\n",
    "module_model = importlib.import_module('torchvision.models')\n",
    "model = getattr(module_model, args.model)(num_classes=ori_dataset.num_class).to(device)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "cm_train = ConfusionMatrix(ori_dataset.num_class)\n",
    "cm_test = ConfusionMatrix(ori_dataset.num_class)\n",
    "global_epoch = 0\n",
    "losses_train = 0\n",
    "losses_test = 0\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "model.train()\n",
    "start_time = datetime.now()\n",
    "train_iter = iter(train_loader)\n",
    "for step in range(args.steps):\n",
    "    try:\n",
    "        img, label = next(train_iter)\n",
    "    except StopIteration:\n",
    "        global_epoch += 1\n",
    "        train_iter = iter(train_loader)\n",
    "        img, label = next(train_iter)\n",
    "    img, label = img.to(device), label.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(img)\n",
    "    loss = loss_fn(y_hat, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred = F.softmax(y_hat, 1).argmax(1)\n",
    "    losses_train += loss.item()\n",
    "    cm_train.update(label.cpu().numpy(), pred.cpu().numpy())\n",
    "    \n",
    "    if (step+1) % 100 == 0:\n",
    "        model.eval()\n",
    "        for i, (img, label) in enumerate(test_loader):\n",
    "            img, label = img.to(device), label.to(device) \n",
    "\n",
    "            y_hat = model(img)\n",
    "            loss = loss_fn(y_hat, label)\n",
    "\n",
    "            pred = F.softmax(y_hat, 1).argmax(1)\n",
    "            losses_test += loss.item()\n",
    "            cm_test.update(label.cpu().numpy(), pred.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        losses_train = losses_train / 100\n",
    "        losses_test = losses_test / len(test_loader)\n",
    "        acc_train = cm_train.getAccuracy()\n",
    "        acc_test = cm_test.getAccuracy()\n",
    "        acc_train_cls = cm_train.getAccuracyPerClass()\n",
    "        acc_test_cls = cm_test.getAccuracyPerClass()\n",
    "        \n",
    "        acc_train_cls = {f'acc/train/{idx}': acc for idx, acc in enumerate(acc_train_cls)}\n",
    "        acc_test_cls = {f'acc/test/{idx}': acc for idx, acc in enumerate(acc_test_cls)}\n",
    "        \n",
    "        if best_acc < acc_test:\n",
    "            best_acc = acc_test\n",
    "            wandb.summary['best/acc'] = acc_test\n",
    "            wandb.summary['best/step'] = step\n",
    "            for idx, acc in enumerate(acc_test_cls):\n",
    "                wandb.summary[f'best/acc/{idx}'] = acc\n",
    "            \n",
    "            \n",
    "        \n",
    "        wandb.log({'loss/train': losses_train,\n",
    "                   'loss/test': losses_test,\n",
    "                   'acc/train': acc_train,\n",
    "                   'acc/test': acc_test}, step=(step+1))\n",
    "        \n",
    "        wandb.log(acc_train_cls, step=(step+1))\n",
    "        wandb.log(acc_test_cls, step=(step+1))\n",
    "        \n",
    "        \n",
    "        print(f'step: {step+1}/{args.steps}({((step+1) / args.steps)*100:.2f}%), '\n",
    "                    f'time: {elapsed_time(start_time)}, '\n",
    "                    f'loss train: {losses_train}, '\n",
    "                    f'loss test: {losses_test}, '\n",
    "                    f'acc train: {acc_train}, '\n",
    "                    f'acc test: {acc_test}, '\n",
    "                    f'acc best: {best_acc}')\n",
    "        \n",
    "        model.train()\n",
    "        losses_train = 0\n",
    "        losses_test = 0\n",
    "        cm_train.reset()\n",
    "        cm_test.reset()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for idx, (acc_train, acc_test) in enumerate(zip(acc_train_cls, acc_test_cls)):\n",
    "        #     print(f'cls {idx}: {acc_train}, {acc_test}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinaenjuni\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dblab/git/ECOGAN/src/wandb/run-20221225_142814-3pm1qdv9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/sinaenjuni/cls_eval/runs/3pm1qdv9\" target=\"_blank\">celestial-smoke-2</a></strong> to <a href=\"https://wandb.ai/sinaenjuni/cls_eval\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sinaenjuni/cls_eval/runs/3pm1qdv9?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f69a0157d10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--model\", type=str, default='resnet18', required=False)\n",
    "parser.add_argument(\"--gpus\", type=int, default=1, required=False)\n",
    "parser.add_argument(\"--data_seq\", type=int, default=0, required=False)\n",
    "parser.add_argument(\"--steps\", type=int, default=1000, required=False)\n",
    "\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001, required=False)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128, required=False)\n",
    "\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "wandb.init(project=\"cls_eval\", entity=\"sinaenjuni\", config=args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAE-BGAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a33dc1eb18f8d9c9688ea8385be21b6e79566cc58fca365bfde15e076f65c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
